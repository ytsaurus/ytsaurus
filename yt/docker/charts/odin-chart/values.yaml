image:
  repository: ghcr.io/ytsaurus/odin
  tag: "0.0.1"
  pullPolicy: IfNotPresent

secretRefs:
  - odin-secrets

config:
  mountPath: /opt/odin

  odin:
    useIPv4: false
    useIPv6: false
    db:
      initialize: true
      proxy: "ytsaurus.http.proxy.tech"
      tokenEnvVariable: "YT_TOKEN"
      table: "//sys/odin/checks"
      tabletCellBundle: "default"
    clusters:
      - proxy: "ytsaurus.http.proxy.tech"
        cluster: "minisaurus"
        tokenEnvVariable: "YT_TOKEN"
    logging:
      port: 10236
      filename: "/dev/stderr"

  webservice:
    host: "::"
    port: 9002
    threadCount: 4
    debug: 0
    logging:
      filename: "/dev/stderr"

  checks:
    sort_result:
      displayName: Sort Result
      enable: true
      config:
        options:
          soft_sort_timeout: 70
          temp_tables_path: "//sys/admin/odin/sort_result"
        check_timeout: 120
    map_result:
      displayName: Map Result
      enable: true
      config:
        options:
          soft_map_timeout: 105
          temp_tables_path: "//sys/admin/odin/map_result"
        check_timeout: 180
    dynamic_table_commands:
      displayName: Dynamic Table Commands
      enable: true
      config:
        options:
          temp_tables_path: "//sys/admin/odin/dynamic_table_commands"
          tablet_cell_bundle: sys
    suspicious_jobs:
      displayName: Suspicious Jobs
      enable: true
      config:
        options:
          critical_suspicious_job_inactivity_timeout: 420
    lost_vital_chunks:
      displayName: Lost Vital Chunks
      enable: true
      config:
        options:
          max_size: 10
    clock_quorum_health:
      displayName: Clock Quorum Health
      # Separate clock servers are not configured in cluster by default.
      enable: false
      config:
        enable: false
        options:
          monitoring_port: 10016
    quorum_health:
      displayName: Quorum Health
      enable: true
      config:
        options:
          monitoring_port: 10010
    tablet_cells:
      displayName: Tablet Cells
      enable: true
      config:
        check_timeout: 120
    tablet_cell_gossip:
      displayName: Tablet Cell Gossip
      enable: true
      config:
        check_timeout: 120
    tablet_cell_snapshots:
      displayName: Tablet Cell Snapshots
      enable: true
      config:
        check_timeout: 120
    scheduler_uptime:
      displayName: Scheduler Uptime
      enable: true
      config:
        check_timeout: 120
    controller_agent_count:
      displayName: Controller Agent Count
      enable: true
      config:
        check_timeout: 120
    controller_agent_uptime:
      displayName: Controller Agent Uptime
      enable: true
      config:
        check_timeout: 120
    operations_satisfaction:
      displayName: Operations Satisfaction
      enable: true
      config:
        options:
          state_path: "//sys/admin/odin/operations_satisfaction_state"
          min_satisfaction_ratio: 0.9
          critical_unsatisfied_minute_count: 10
    operations_snapshots:
      displayName: Operations Snapshots
      enable: true
      config:
        enable: true
        options:
          critical_time_without_snapshot_threshold: 3600
    operations_count:
      displayName: Operations Count
      enable: true
      config:
        options:
          operations_count_threshold: 5000
          recoursive_node_count_warn: 40000
          recoursive_node_count_crit: 60000
    dynamic_table_replication:
      displayName: Dynamic Table Replication
      # This check applicable for group of clusters with dynamic table replication.
      enable: false
      config:
        enable: false
    register_watcher:
      displayName: Register Watcher
      enable: true
      config:
        options:
          crit_threshold: 7
          warn_threshold: 5
    tmp_node_count:
      displayName: Tmp Node Count
      enable: true
      config:
        options:
          file_count_threshold: 40000
          table_count_threshold: 40000
          root_count_threshold: 40000
    destroyed_replicas_size:
      displayName: Destroyed Replicas Size
      enable: true
      config:
        options:
          node_threshold: 100000
          total_threshold: 1000000
    query_tracker_yql_liveness:
      displayName: Query Tracker Yql Liveness
      enable: true
      config:
        check_timeout: 240
        options:
          soft_query_timeout: 165
          cluster_name_to_query_tracker_stage: {}
    query_tracker_chyt_liveness:
      displayName: Query Tracker Chyt Liveness
      # This check applicable only if default CHYT clique is configured.
      enable: false
      config:
        check_timeout: 60
        options:
          soft_query_timeout: 30
    query_tracker_ql_liveness:
      displayName: Query Tracker Ql Liveness
      enable: false
      config:
        check_timeout: 60
        options:
          soft_query_timeout: 30
    query_tracker_dq_liveness:
      displayName: Query Tracker Dq Liveness
      # This check applicable only if DQ is configured.
      enable: false
      config:
        check_timeout: 60
        options:
          soft_query_timeout: 30
    bundle_hotfix:
      displayName: Bundle Hotfix
      enable: true
      config: {}
    controller_agent_alerts:
      displayName: Controller Agent Alerts
      enable: true
      config: {}
    controller_agent_operation_memory_consumption:
      displayName: Controller Agent Operation Memory Consumption
      enable: true
      config: {}
    discovery:
      displayName: Discovery
      # This check applicable only for cluster with discovery servers.
      enable: false
      config: {}
    master:
      displayName: Master
      enable: true
      config: {}
    master_alerts:
      displayName: Master Alerts
      enable: true
      config: {}
    master_chunk_management:
      displayName: Master Chunk Management
      enable: true
      config: {}
    medium_balancer_alerts:
      displayName: Medium Balancer Alerts
      enable: true
      config: {}
    missing_part_chunks:
      displayName: Missing Part Chunks
      enable: true
      config: {}
    oauth_health:
      displayName: Oauth Health
      enable: true
      config: {}
    operations_archive_tablet_store_preload:
      displayName: Operations Archive Tablet Store Preload
      enable: true
      config: {}
    proxy:
      displayName: Proxy
      enable: true
      config: {}
    queue_api:
      displayName: Queue Api
      # Queue agents are not configured in cluster by default.
      enable: false
      config:
        options:
          temp_tables_path: "//sys/admin/odin/queue_api"
          tablet_cell_bundle: "sys"
          wait_timeout: 15
    queue_agent_alerts:
      displayName: Queue Agent Alerts
      # Queue agents are not configured in cluster by default.
      enable: false

      config:
        options:
          cluster_name_to_query_tracker_count: {}
          queue_agent_stage_clusters: {}
    query_tracker_alerts:
      displayName: Query Tracker Alerts
      enable: true
      config:
        options:
          cluster_name_to_query_tracker_count: {}
    scheduler:
      displayName: Scheduler
      enable: true
      config: {}
    scheduler_alerts:
      displayName: Scheduler Alerts
      enable: true
      config: {}
    scheduler_alerts_jobs_archivation:
      displayName: Scheduler Alerts Jobs Archivation
      enable: true
      config: {}
    scheduler_alerts_update_fair_share:
      displayName: Scheduler Alerts Update Fair Share
      enable: true
      config: {}
    stuck_missing_part_chunks:
      displayName: Stuck Missing Part Chunks
      enable: true
      config: {}
    chaos_cells:
      displayName: Chaos Cells
      enable: true
      config: {}
    unaware_nodes:
      displayName: Unaware Nodes
      # This check applicable only for large cluster with nodes distributed by racks.
      enable: false
      config: {}

odin:
  replicaCount: 1
  command: ["yt_odin"]
  args: ["--config", "/opt/odin/odin_config.json"]
  env: []
  envFrom: []
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
  extraVolumeMounts: []
  extraVolumes: []
  serviceAccount:
    create: true
    name: ""
  initJob:
    backoffLimit: 2
    resources: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}


webservice:
  replicaCount: 1
  command: ["yt_odin_webservice"]
  args: ["--config", "/opt/odin/odin_webservice_config.json"]
  env: []
  envFrom: []
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  podAnnotations: {}
  extraVolumeMounts: []
  extraVolumes: []
  service:
    type: ClusterIP
    port: 9002

metrics:
  enable: true
  serviceMonitor:
    enable: false
    labels:
      release: kube-prom
