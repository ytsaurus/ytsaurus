#!/usr/bin/env python

from spyt.dependency_utils import require_yt_client
require_yt_client()

from yt.wrapper import YtClient  # noqa: E402
from yt.wrapper.cli_helpers import ParseStructuredArgument  # noqa: E402
from yt.wrapper.http_helpers import get_user_name  # noqa: E402
from spyt.standalone import start_spark_cluster, SparkDefaultArguments, SpytEnablers  # noqa: E402
from spyt import utils as spark_utils  # noqa: E402


def add_group(parser, enabler_arg, disabler_arg, dest_arg, default_value):
    group = parser.add_mutually_exclusive_group(required=False)
    group.add_argument(enabler_arg, dest=dest_arg, action='store_true')
    group.add_argument(disabler_arg, dest=dest_arg, action='store_false')
    parser.set_defaults(**{dest_arg: default_value})


def main(raw_args=None):
    parser = spark_utils.get_default_arg_parser(description="Spark Launch")

    parser.add_argument("--worker-cores", required=True, type=int)
    parser.add_argument("--worker-memory", required=True)
    parser.add_argument("--worker-num", required=True, type=int)
    parser.add_argument("--worker-cores-overhead", required=False, type=int,
                        default=SparkDefaultArguments.SPARK_WORKER_CORES_OVERHEAD)
    parser.add_argument("--worker-memory-overhead", required=False,
                        default=SparkDefaultArguments.SPARK_WORKER_MEMORY_OVERHEAD)
    parser.add_argument("--worker-timeout", required=False, default=SparkDefaultArguments.SPARK_WORKER_TIMEOUT)
    parser.add_argument("--pool", required=False)
    parser.add_argument("--tmpfs-limit", required=False, default=SparkDefaultArguments.SPARK_WORKER_TMPFS_LIMIT)
    # COMPAT(alex-shishkin): replace with worker-disk-name='ssd_slots_physical' and worker-disk-limit
    parser.add_argument("--ssd-limit", required=False, default=None)
    # COMPAT(alex-shishkin): replace with worker-disk-name='ssd_slots_physical' and worker-disk-account
    parser.add_argument("--ssd-account", required=False, default=None)
    parser.add_argument("--worker-disk-name", required=False, default="default")
    parser.add_argument("--worker-disk-limit", required=False, default=None)
    parser.add_argument("--worker-disk-account", required=False, default=None)
    parser.add_argument("--master-memory-limit", required=False,
                        default=SparkDefaultArguments.SPARK_MASTER_MEMORY_LIMIT)
    parser.add_argument("--history-server-memory-limit",
                        required=False, default=SparkDefaultArguments.SPARK_HISTORY_SERVER_MEMORY_LIMIT)
    parser.add_argument("--history-server-memory-overhead",
                        required=False, default=SparkDefaultArguments.SPARK_HISTORY_SERVER_MEMORY_OVERHEAD)
    parser.add_argument("--history-server-cpu-limit",
                        required=False, default=SparkDefaultArguments.SPARK_HISTORY_SERVER_CPU_LIMIT, type=int)
    parser.add_argument("--operation-alias", required=False)
    parser.add_argument("--network-project", required=False)
    parser.add_argument("--params", required=False, action=ParseStructuredArgument, dest="params",
                        default=SparkDefaultArguments.get_params())
    parser.add_argument('--abort-existing', required=False,
                        action='store_true', default=False)
    parser.add_argument("--spyt-version", "--spark-cluster-version", required=False)
    parser.add_argument("--worker-log-update-interval",
                        required=False, default=SparkDefaultArguments.SPARK_WORKER_LOG_UPDATE_INTERVAL)
    parser.add_argument("--worker-log-table-ttl", required=False,
                        default=SparkDefaultArguments.SPARK_WORKER_LOG_TABLE_TTL)
    parser.add_argument("--shs-location", required=False)
    parser.add_argument("--preemption_mode", required=False, default="normal")
    parser.add_argument("--cluster-log-level", required=False, default="INFO")

    parser.add_argument('--enable-multi-operation-mode',
                        dest='enable_multi_operation_mode', action='store_true')
    parser.add_argument('--disable-multi-operation-mode',
                        dest='enable_multi_operation_mode', action='store_false')
    parser.set_defaults(enable_multi_operation_mode=False)

    default_enablers = SpytEnablers()
    add_group(parser, '--enable-byop', '--disable-byop', 'enable_byop', default_enablers.enable_byop)
    add_group(parser, '--enable-solomon-agent', '--disable-solomon-agent', 'enable_solomon_agent',
              default_enablers.enable_solomon_agent)
    add_group(parser, '--enable-profiling', '--disable-profiling', 'enable_profiling',
              default_enablers.enable_profiling)
    add_group(parser, '--enable-mtn', '--disable-mtn', 'enable_mtn', default_enablers.enable_mtn)
    add_group(parser, '--prefer-ipv6', '--prefer-ipv4', 'enable_preference_ipv6',
              default_enablers.enable_preference_ipv6)

    add_group(parser, '--enable-tmpfs', '--disable-tmpfs', 'enable_tmpfs', True)
    add_group(parser, '--enable-stderr-table', '--disable-stderr-table', 'enable_stderr_table', False)

    add_group(parser, '--enable-tcp-proxy', '--disable-tcp-proxy', 'enable_tcp_proxy',
              default_enablers.enable_tcp_proxy)
    parser.add_argument('--tcp-proxy-range-start', required=False, default=30000, type=int)
    parser.add_argument('--tcp-proxy-range-size', required=False, default=100, type=int)

    add_group(parser, '--enable-history-server', '--disable-history-server', 'enable_history_server', True)

    add_group(parser, '--enable-advanced-event-log', '--disable-advanced-event-log', 'advanced_event_log', True)

    add_group(parser, '--enable-rpc-job-proxy', '--disable-rpc-job-proxy', 'rpc_job_proxy', False)
    parser.add_argument("--rpc-job-proxy-thread-pool-size", required=False, default=4, type=int)

    add_group(parser, '--enable-worker-log-transfer', '--disable-worker-log-transfer', 'worker_log_transfer', False)
    add_group(parser, '--enable-worker-log-json-mode', '--disable-worker-log-json-mode', 'worker_log_json_mode', False)

    add_group(parser, '--enable-dedicated-driver-operation-mode', '--disable-dedicated-driver-operation-mode',
              'dedicated_operation_mode', False)

    subgroup = parser.add_argument_group(
        '(experimental) run driver in dedicated operation')
    subgroup.add_argument("--driver-cores", required=False,
                          type=int, help="same as worker-cores by default")
    subgroup.add_argument("--driver-memory", required=False,
                          type=int, help="same as worker-memory by default")
    subgroup.add_argument("--driver-num", required=False,
                          type=int, help="Number of driver workers")
    subgroup.add_argument("--driver-cores-overhead", required=False,
                          type=int, help="same as worker-cores-overhead by default")
    subgroup.add_argument("--driver-timeout", required=False,
                          help="same as worker-timeout by default")

    subgroup = parser.add_argument_group("(experimental) autoscaler")
    subgroup.add_argument("--autoscaler-period", required=False,
                          type=str,
                          help="""
                            Start autoscaler process with provided period between autoscaling actions.
                            Period format is '<number> <time_unit>', for example '1s', '5 seconds', '100millis' etc.
                          """.strip())
    subgroup.add_argument("--autoscaler-metrics-port", required=False,
                          type=int, help="expose autoscaler metrics on provided port")
    subgroup.add_argument("--autoscaler-sliding-window", required=False,
                          type=int, help="size of autoscaler actions sliding window (in number of action) to downscale")
    subgroup.add_argument("--autoscaler-max-free-workers", required=False,
                          type=int, help="autoscaler maximum number of free workers")
    subgroup.add_argument("--autoscaler-slot-increment-step", required=False,
                          type=int, help="autoscaler worker slots increment step")

    subgroup = parser.add_argument_group("Livy server")
    subgroup.add_argument('--enable-livy', action='store_true', default=False)
    subgroup.add_argument('--livy-driver-cores', required=False,
                          default=SparkDefaultArguments.LIVY_DRIVER_CORES, type=int)
    subgroup.add_argument('--livy-driver-memory', required=False, default=SparkDefaultArguments.LIVY_DRIVER_MEMORY)
    subgroup.add_argument('--livy-max-sessions', required=False,
                          default=SparkDefaultArguments.LIVY_MAX_SESSIONS, type=int)

    args, unknown_args = spark_utils.parse_args(parser, raw_args=raw_args)

    yt_client = YtClient(proxy=args.proxy, token=spark_utils.default_token())

    if args.autoscaler_period and not args.enable_multi_operation_mode:
        print("Autoscaler could be enabled only with multi-operation mode")
        exit(-1)

    start_spark_cluster(worker_cores=args.worker_cores,
                        worker_memory=args.worker_memory,
                        worker_num=args.worker_num,
                        worker_cores_overhead=args.worker_cores_overhead,
                        worker_memory_overhead=args.worker_memory_overhead,
                        worker_timeout=args.worker_timeout,
                        operation_alias=args.operation_alias,
                        discovery_path=args.discovery_path,
                        pool=args.pool or get_user_name(client=yt_client),
                        enable_tmpfs=args.enable_tmpfs,
                        tmpfs_limit=args.tmpfs_limit,
                        ssd_limit=args.ssd_limit,
                        ssd_account=args.ssd_account,
                        worker_disk_name=args.worker_disk_name,
                        worker_disk_limit=args.worker_disk_limit,
                        worker_disk_account=args.worker_disk_account,
                        master_memory_limit=args.master_memory_limit,
                        enable_history_server=args.enable_history_server,
                        history_server_memory_limit=args.history_server_memory_limit,
                        history_server_memory_overhead=args.history_server_memory_overhead,
                        history_server_cpu_limit=args.history_server_cpu_limit,
                        network_project=args.network_project,
                        tvm_id=spark_utils.default_tvm_id(),
                        tvm_secret=spark_utils.default_tvm_secret(),
                        abort_existing=args.abort_existing,
                        advanced_event_log=args.advanced_event_log,
                        worker_log_transfer=args.worker_log_transfer,
                        worker_log_json_mode=args.worker_log_json_mode,
                        worker_log_update_interval=args.worker_log_update_interval,
                        worker_log_table_ttl=args.worker_log_table_ttl,
                        params=args.params,
                        shs_location=args.shs_location,
                        spark_cluster_version=args.spyt_version,
                        enablers=SpytEnablers(
                            enable_byop=args.enable_byop,
                            enable_profiling=args.enable_profiling,
                            enable_mtn=args.enable_mtn,
                            enable_solomon_agent=args.enable_solomon_agent,
                            enable_preference_ipv6=args.enable_preference_ipv6,
                            enable_tcp_proxy=args.enable_tcp_proxy
                        ),
                        client=yt_client,
                        preemption_mode=args.preemption_mode,
                        cluster_log_level=args.cluster_log_level,
                        enable_multi_operation_mode=args.enable_multi_operation_mode,
                        dedicated_operation_mode=args.dedicated_operation_mode,
                        driver_cores=args.driver_cores,
                        driver_memory=args.driver_memory,
                        driver_num=args.driver_num,
                        driver_cores_overhead=args.driver_cores_overhead,
                        driver_timeout=args.driver_timeout,
                        autoscaler_period=args.autoscaler_period,
                        autoscaler_metrics_port=args.autoscaler_metrics_port,
                        autoscaler_sliding_window=args.autoscaler_sliding_window,
                        autoscaler_max_free_workers=args.autoscaler_max_free_workers,
                        autoscaler_slot_increment_step=args.autoscaler_slot_increment_step,
                        enable_livy=args.enable_livy,
                        livy_driver_cores=args.livy_driver_cores,
                        livy_driver_memory=args.livy_driver_memory,
                        livy_max_sessions=args.livy_max_sessions,
                        rpc_job_proxy=args.rpc_job_proxy,
                        rpc_job_proxy_thread_pool_size=args.rpc_job_proxy_thread_pool_size,
                        tcp_proxy_range_start=args.tcp_proxy_range_start,
                        tcp_proxy_range_size=args.tcp_proxy_range_size,
                        enable_stderr_table=args.enable_stderr_table)


if __name__ == '__main__':
    main()
