package NYT.NScheduler.NProto;

import "yt/ytlib/node_tracker_client/node.proto";
import "yt/ytlib/job_tracker_client/job.proto";
import "yt/ytlib/chunk_client/chunk_meta.proto";
import "yt/ytlib/chunk_client/chunk_spec.proto";
import "yt/ytlib/chunk_client/schema.proto";
import "yt/ytlib/chunk_client/chunk_owner_ypath.proto";
import "yt/ytlib/new_table_client/chunk_meta.proto";
import "yt/ytlib/file_client/file_ypath.proto";
import "yt/core/misc/error.proto";
import "yt/core/misc/guid.proto";

////////////////////////////////////////////////////////////////////////////////

message TRegularFileDescriptor
{
    required bool executable = 2;
    required string file_name = 3;
    repeated NChunkClient.NProto.TChunkSpec chunks = 4;
}

message TTableFileDescriptor
{
    required bytes format = 2;
    required string file_name = 3;
    repeated NChunkClient.NProto.TChunkSpec chunks = 4;
}

// Specification for starting user code during a job.
message TUserJobSpec
{
    // Additional files to be placed into the sandbox.
    repeated TRegularFileDescriptor regular_files = 1;

    // Additional tables to be placed into the sandbox.
    repeated TTableFileDescriptor table_files = 6;

    // The user command to be executed.
    required string shell_command = 2;

    // Input format description (in YSON).
    required string input_format = 3;

    // Output format description (in YSON).
    required string output_format = 4;

    // Environment strings (K=V format) for starting user process.
    repeated string environment = 5;

    // Hard memory limit for user process, in bytes.
    required int64 memory_limit = 7;

    // Memory size reserved at job launch, in bytes.
    required int64 memory_reserve = 9;

    // Transaction for creating stderr chunks.
    // If not set then no stderrs are captured.
    optional NYT.NProto.TGuid stderr_transaction_id = 8;

    optional bool use_yamr_descriptors = 10 [default = false];

    required int64 max_stderr_size = 11;

    optional bool enable_core_dump = 12 [default = false];

    // Deprecated = 13.

    optional bool enable_io_prio = 14 [default = true];

    optional bool enable_accounting = 15 [default = false];

    optional bool check_input_fully_consumed = 16 [default = false];

    required NYT.NNodeTrackerClient.NProto.TNodeDirectory node_directory = 17;
}

// User code result.
message TUserJobResult
{
    // Used for reordering chunks from operations that produce sorted output.
    repeated NVersionedTableClient.NProto.TBoundaryKeysExt output_boundary_keys = 3;
}

////////////////////////////////////////////////////////////////////////////////

// Describes a part of input table(s) to be processed by a job.
message TTableInputSpec
{
    // Chunks comprising the input.
    repeated NYT.NChunkClient.NProto.TChunkSpec chunks = 1;
}

// Defines how to store output from a job into a table.
message TTableOutputSpec
{
    // The chunk list where output chunks must be placed.
    required NYT.NProto.TGuid chunk_list_id = 2;

    // YSON-serialized writer options obtained from table attributes.
    required string table_writer_options = 6;

    optional NVersionedTableClient.NProto.TKeyColumnsExt key_columns = 7;
}

// Describes a job submitted by a scheduler.
message TSchedulerJobSpecExt
{
    extend NJobTrackerClient.NProto.TJobSpec
    {
        optional TSchedulerJobSpecExt scheduler_job_spec_ext = 100;
    }

    // Configuration for IO during job execution.
    required bytes io_config = 1;

    // The transaction used for writing output chunks.
    required NYT.NProto.TGuid output_transaction_id = 2;

    // Job input.
    repeated TTableInputSpec input_specs = 3;

    // Job output.
    repeated TTableOutputSpec output_specs = 4;

    // Maps node ids to descriptors for input chunks.
    required NNodeTrackerClient.NProto.TNodeDirectory node_directory = 5;

    // Total input uncompressed data size estimate.
    optional int64 input_uncompressed_data_size = 6 [default = 0];

    // Total input row count estimate.
    optional int64 input_row_count = 7 [default = 0];

    required int64 lfalloc_buffer_size = 8;

    // True if data_size and row_count are approximate (e.g. restarted sort jobs).
    optional bool is_approximate = 9 [default = false];

    optional bool enable_job_proxy_memory_control = 10 [default = true];

    optional bool enable_sort_verification = 11 [default = true];

    optional TUserJobSpec user_job_spec = 12;
}

message TSchedulerJobResultExt
{
    extend NJobTrackerClient.NProto.TJobResult
    {
        optional TSchedulerJobResultExt scheduler_job_result_ext = 100;
    }

    // Provides node id to descriptor mapping for |chunks|.
    optional NYT.NNodeTrackerClient.NProto.TNodeDirectory node_directory = 2;

    // List of output chunks produced by the job.
    // NB: Only filled when necessary.
    repeated NYT.NChunkClient.NProto.TChunkSpec chunks = 3;

    optional NYT.NProto.TGuid stderr_chunk_id = 4;

    // List of input chunks the job was unable to read.
    repeated NYT.NProto.TGuid failed_chunk_ids = 5;

    repeated NYT.NProto.TGuid fail_context_chunk_ids = 6;

    optional TUserJobResult user_job_result = 7;
}

////////////////////////////////////////////////////////////////////////////////

// Map jobs.
/*
 * Conceptually map is the simplest operation.
 * Input consists of a number of tables (or parts thereof).
 * These tables are merged together into a sequence of rows,
 * sequence is split into fragments and these fragments
 * are fed to jobs. Each job runs a given shell command.
 * The outputs of jobs are collected thus forming a number
 * of output tables.
 *
 * The input spec must contain TUserJobSpec.
 * The result must contain TUserJobResult.
 */

////////////////////////////////////////////////////////////////////////////////

// Merge jobs.
/*
 * A merge job takes a number of chunks sequences (each containing sorted data)
 * and merges them. The result is split into chunks again.
 *
 * The input spec should contain TMergeJobSpecExt.
 *
 */

message TMergeJobSpecExt
{
    extend NJobTrackerClient.NProto.TJobSpec
    {
        optional TMergeJobSpecExt merge_job_spec_ext = 102;
    }

    // For EJobType::SortedMerge, contains columns used for comparison.
    repeated string key_columns = 1;
}

////////////////////////////////////////////////////////////////////////////////

// Partition jobs.
/*
 * A partition jobs read the input and scatters the rows into buckets depending
 * on their keys. When a bucket becomes full, it is written as a block.
 * Output blocks are marked with |partition_tag| to enable subsequently
 * started sort jobs to fetch appropriate portions of data.
 *
 * The input spec should contain TPartitionJobSpecExt.
 * The result must contain TSchedulerJobResultExt.
 *
 */

message TPartitionJobSpecExt
{
    extend NJobTrackerClient.NProto.TJobSpec
    {
        optional TPartitionJobSpecExt partition_job_spec_ext = 103;
    }

    // Number of partitions.
    required int32 partition_count = 1;

    // Deprecated = 2.
    // Deprecated = 3.
    // Deprecated = 4.

    // If empty then THashPartitioner is used.
    // Otherwise TOrderedPartitioner is used.

    repeated string partition_keys = 5;

    // Sort key columns (may be wider than reduce key).
    required NVersionedTableClient.NProto.TKeyColumnsExt sort_key_columns = 6;

    // Size of key prefix used for bucket decision.
    required int32 reduce_key_column_count = 7;
}

////////////////////////////////////////////////////////////////////////////////

// Sort jobs.
/*
 * A sort job reads the input chunks, sorts the rows, and then flushes
 * the rows into a sequence of output chunks.
 *
 * The input spec should contain TSortJobSpecExt.
 *
 */
message TSortJobSpecExt
{
    extend NJobTrackerClient.NProto.TJobSpec
    {
        optional TSortJobSpecExt sort_job_spec_ext = 104;
    }

    repeated string key_columns = 5;
}

////////////////////////////////////////////////////////////////////////////////

// Reduce jobs.
/*
 * "Everything is either a sort or a merge. Reduce is the latter." (c) Pavel Sushin
 *
 * The input spec should contain TReduceJobSpecExt.
 *
 */

message TReduceJobSpecExt
{
    extend NJobTrackerClient.NProto.TJobSpec
    {
        optional TReduceJobSpecExt reduce_job_spec_ext = 105;
    }

    repeated string key_columns = 1;
}

////////////////////////////////////////////////////////////////////////////////

// Remote copy jobs.

message TRemoteCopyJobSpecExt
{
    extend NJobTrackerClient.NProto.TJobSpec
    {
        optional TRemoteCopyJobSpecExt remote_copy_job_spec_ext = 106;
    }

    required string connection_config = 1;
    optional NYT.NNodeTrackerClient.NProto.TNodeDirectory remote_node_directory = 2;
    required string network_name = 3;
}

////////////////////////////////////////////////////////////////////////////////
